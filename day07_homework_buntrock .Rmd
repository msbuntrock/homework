---
title: "Day 7 Homework"
output: html_document
---

NOTE: this counts for both Day 6 and Day 7 homework

1. Create a small (fake) data set that gives some extra data on some kind of group in your data. For example, person-level data on subjects, country-level data if you have countries, something like that.  In this data frame, leave some values out, so that (for example) some of the people/countries/whatever in your primary data are missing from this smaller data set.  Conversely, add some additional values in the small data (for example, some people that are not in your primary data).

```{r}
library(ggplot2)
library(tidyverse)
library(reshape2)
library(carData)
data(Vocab)

voc.year <-  subset(Vocab, Vocab$year == 2016) 

voc.year <- subset(Vocab, Vocab$year == 2016) & people.dat (rep(372.2))

```

2. Practice merging this data together. First, perform a "full join", where you don't lose any of the values from EITHER data set. Do this with both base R and dplyr functions. Check the # of rows in the new data set, and try to understand why it increased as much as it did.

```{r}
v.fulljoin <- full_join(voc.year, Vocab)

nrow(v.fulljoin)
```


3. Now try an "inner join", where you only end up with the data where the two data sets overlap. Again, both base R and dplyr methods.

```{r}
v.innerjoin <- inner_join(voc.year, Vocab)

nrow(v.fulljoin)
```



4. Now try a "left join" and a "right join". In a left join, the all of the cases (rows) of the "left-hand" data set are retained, and the other data is only merged in "where available". For example, if you do a left join with your primary data set as the "left hand" data, you should end up with all the original data, but only a subset of the new data will be merged in.  A "right join" is the same concept, just that it's the "right hand" data set that is retained in full. In the `dplyr` functions, "left" is just the first data frame argument, and "right" is the second.  Try both, and examine the results to make sure it makes sense.

```{r}
v.leftjoin <- left_join(voc.year, Vocab)

v.rightjoin <- right_join(voc.year, Vocab)

```


5. Find some other aspect of your data that needs to be combined or merged in some way.  This could be a variety of things:
  - multiple "sheets" in an Excel
  - multiple files that should be combined into one data set
  - properties that need to be merged in to one data set. For example:
    - you have a table of subject properties (gender, handedness, etc.) that need to be merged into the item-level data
    - you have other information on countries (population, etc.) that need to be merged in
  - if you DON'T have this naturally-occurring issue, figure out a way to manufacture it. For example:
    - split up your data into chunks
    - make up some new table of information that you want to merge in
  - Explain (in words) what you are trying to do
  - Try it!
  - Perform several checks to confirm whether it worked or not
  - If it didn't work as intended, describe what happened, and see if you can figure out why
  
```{r}

Vocab$Race <- NA
Vocab$Sex <- merge(Vocab$Sex , Vocab$Race, nreps(100) )
Vocab$comb <- cbind(Vocab$Sex, Vocab$Race)

```

6. Use ggplot to plot a histogram of some variable in your data. Anything unexpected?

```{r}
ggplot(Vocab, aes(vocabulary, education ))+geom_histogram(stat = "Identity")

```


7. Think about the "shape" of your data. Is it "long" or "wide"? Some of both?  Try to identify a way in which it could be make "longer" (i.e., by collapsing two or more columns into a single column along with a "grouping" column) or "wider" (i.e., by splitting out a column across multiple new columns, by some grouping factor).  Use `gather()` and `spread()` from the `tidyr` package to go back and forth between long and wide shapes.

```{r}
voc.long <- gather(Vocab, edscore, composite, education: vocabulary)
head(voc.long)

voc.wide <- spread(voc.long, edscore, composite)
```


8. Make your data "wide" enough that there are two numeric columns that might have a relationship between them. Use ggplot and `geom_point()` to plot a scatterplot of these data.

```{r}
ggplot(Vocab, aes(vocabulary, education))+ geom_point()+ geom_smooth(method = "lm")
```


9. Identify some other "grouping" variable (i.e., factor column) in your data. Re-do the scatterplot from above, but in addition try mapping this additional factor to some other aesthetic, such as `color`, `fill`, `shape`, `size`, etc.

```{r}
ggplot(Vocab, aes(vocabulary, education)) + geom_point(fill = "yellow")
```


10. Now try splitting the scatterplot into multiple plots based on this grouping factor (or a different one). Use `facet_wrap` or `facet_grid` in ggplot.

```{r}
ggplot(Vocab, aes(vocabulary, education)) + geom_point(fill = "yellow") + facet_wrap(~ sex)
```

11. Return to a plot that uses color (if you haven't made one yet, make one now).  Now use the appropriate scale (either `color` or `fill`) to change the colors being plotted.

```{r}
ggplot(Vocab, aes(vocabulary, education)) + geom_violin(fill = "yellow") + facet_wrap(~ sex)
```

12. Identify some aspect of your data where computing something like "cell means" makes sense. For example, getting the mean (or median, or whatever) value of some column, broken down by some grouping factors.  Use the reshaping, grouping, and summarizing functions in `tidyr` and `dplyr` to create a data frame that is a table of these cell means.

```{r}
summary.voc<- summarize(group_by(Vocab, sex), 
                        mean.rating = mean(vocabulary, na.rm = TRUE), sd.rating = sd(vocabulary, na.rm = TRUE),
          N = length(vocabulary))
```

13. Now create a function that computes the standard error of a mean. This is the same as what we did in class, but try to do it from scratch before looking back at the code, to see if you can remember how to do it.

```{r}
se.of.vocab <- function(x) {
  sd(x)/sqrt(length(x)) 
}

```


14. Now re-run the code that summarized your data into cell means, but add the code to also compute the standard errors at the same time, so that you end up with a table of means and standard errors.

```{r}

summary.voc<- summarize(group_by(Vocab, sex), 
                        mean.rating = mean(vocabulary, na.rm = TRUE), sd.rating = sd(vocabulary, na.rm = TRUE),
          N = length(vocabulary), se = se.of.vocab(vocabulary))
```


15.  Now plot your table of means and standard errors, using geom_point() and geom_errorbar(), or using geom_pointrange(), or another geom of your choice.  Try to use +/- 1.96 times the standard error to set the upper and lower bounds of the error bar, because this will essentially give you a 95% confidence interval.

```{r}
ggplot(summary.voc, aes(sex,vocabulary))+geom_pointrange(aes(ymin = mean.rating - 
                                              se*1.96, ymax = mean.rating + se*1.96))

```


16.  Find some additional factor that you'd like to group these means by.  For example, if your first table of means was broken down by experimental condition, now you could try to break it down by both condition and participant gender.  Re-use your code from above to get the cell means & standard errors, but add this new factor to the grouping statement to get the new, more complex table of cell means.

```{r}
summary.voc.2<- summarize(group_by(Vocab, sex, year), 
                        mean.rating = mean(vocabulary, na.rm = TRUE), sd.rating = sd(vocabulary, na.rm = TRUE),
          N = length(vocabulary), se = se.of.vocab(vocabulary))

head(summary.voc.2)
summary.voc.2

```


17. Plot this new table of cell means, and map the new factor to an additional aesthetic, like color, or use it to facet your plot.  That is, this plot should look similar to the plot from #14 above, but also broken down by your new factor in some way.

```{r}
ggplot(summary.voc.2, aes(year,mean.rating))+geom_pointrange(aes(ymin = mean.rating - 
                                              se*1.96, ymax = mean.rating + se*1.96))+facet_wrap(~sex)

```


18. Save both plots in #14 and #17 into a single PDF, and save the two cell means tables as (separate) CSV files.

```{r}
#done 




```

